{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b452a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import BicScore\n",
    "from pgmpy.base import DAG\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pgmpy.base import DAG\n",
    "from rpy2.robjects import r\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects import globalenv\n",
    "from rpy2.robjects.packages import importr\n",
    "from pgmpy.estimators import K2Score\n",
    "from rpy2.robjects import conversion, default_converter, pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "import rpy2.robjects as ro\n",
    "from pgmpy.estimators import ExpectationMaximization as EM\n",
    "from pgmpy.estimators import BayesianEstimator as BE\n",
    "pandas2ri.activate()\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b37bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(edges_dag,columns):\n",
    "    #将图转换为矩阵\n",
    "    edges_matrix = pd.DataFrame(np.zeros((len(columns),len(columns))),index = columns,columns=columns).astype(int)\n",
    "    for edges in edges_dag:\n",
    "        edges_matrix.loc[edges[0],edges[1]]+=1\n",
    "    return edges_matrix\n",
    "def to_dag(edges_matrix):\n",
    "    edges_list = []\n",
    "    for index in edges_matrix.index:\n",
    "        for col in edges_matrix.columns:\n",
    "            if edges_matrix.loc[index,col]!=0:\n",
    "                edges_list.append((index,col))\n",
    "            \n",
    "    #将矩阵转换为图\n",
    "    return edges_list\n",
    "# Funtion to evaluate the learned model structures.\n",
    "def get_f1_score(estimated_model, true_model):\n",
    "    nodes = estimated_model.nodes()\n",
    "    est_adj = nx.to_numpy_matrix(\n",
    "        estimated_model.to_undirected(), nodelist=nodes, weight=None\n",
    "    )\n",
    "    true_adj = nx.to_numpy_matrix(\n",
    "        true_model.to_undirected(), nodelist=nodes, weight=None\n",
    "    )\n",
    "\n",
    "    f1 = f1_score(np.ravel(true_adj), np.ravel(est_adj))\n",
    "    print(\"F1-score for the model skeleton: \", f1)    \n",
    "def print_dag(dag):\n",
    "    if dag.has_node(\"label\"):\n",
    "        for edges in dag.get_ancestral_graph(\"label\").edges:\n",
    "            print(edges[0],\"->\",edges[1],\";\")\n",
    "    else:\n",
    "        for edges in dag.edges:\n",
    "            print(edges[0],\"->\",edges[1],\";\")\n",
    "def model_auc(prob,y_true):\n",
    "    y_prob = []\n",
    "    num = 0\n",
    "    for i in range(len(prob)):\n",
    "        y_prob.append(prob.iloc[i,y_true[i]])\n",
    "    if y_predic[i]==y_true[i]:\n",
    "        num+=1\n",
    "    fpr, tpr, thresholds = roc_curve(y_true,y_predic)\n",
    "    \n",
    "    return auc(fpr, tpr)\n",
    "def to_strength_list(edges_strength_martix):\n",
    "    str_list = []\n",
    "    for col in edges_strength_martix.columns:\n",
    "        for idx in edges_strength_matrix.columns:\n",
    "            if edges_strength_matrix[col][idx] < 0:\n",
    "                str_list.append([idx,col,edges_strength_matrix[col][idx]])\n",
    "    return str_list\n",
    "def print_edges(ed):\n",
    "    for i in ed:\n",
    "        print(i[0],\"->\",i[1],\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174fa9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class client():\n",
    "    \"\"\"\n",
    "    只改动模型训练部分\n",
    "    black和whitelist 的名单修改\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,data,cid,strength):\n",
    "        #black 用R语言的存储方式存储·\n",
    "        self.data = data\n",
    "        self.cid = cid\n",
    "        #初始化创建一个空的图\n",
    "        self.model = DAG()\n",
    "        self.edges_strength =  strength\n",
    "        \n",
    "    def get_model(self):\n",
    "        #获得本地客户端网络结构\n",
    "        return self.model\n",
    "    def fit(self):\n",
    "        #记录最好的模型\n",
    "        # 代改动部分        \n",
    "        \n",
    "        self_r_data = ro.conversion.py2rpy(self.data)\n",
    "        globalenv['self_r_data'] = self_r_data\n",
    "        r_script=\"\"\"\n",
    "        library(bnlearn)\n",
    "        \n",
    "        \n",
    "        datacols = names(self_r_data)\n",
    "        for (i in 1:ncol(self_r_data)) { \n",
    "        if(i>45){\n",
    "            self_r_data[,datacols[i]] <- factor(self_r_data[,datacols[i]],levels = c(0,1))\n",
    "        }else{\n",
    "            self_r_data[,datacols[i]] <- factor(self_r_data[,datacols[i]])\n",
    "              }\n",
    "        }\n",
    "        dag = hc(self_r_data,score = sc,blacklist = bl_r)\n",
    "        arcs = arc.strength(dag,self_r_data)\n",
    "        \n",
    "        \"\"\"\n",
    "        edges = r(r_script)\n",
    "        self.arc_strength_list = pd.DataFrame(edges)\n",
    "        self.model_edges = edges\n",
    "        #创建新的模型，并添加结果\n",
    "        self.model = DAG()\n",
    "        for i in range(len(edges[\"from\"])):\n",
    "            #汇集频数\n",
    "            self.model.add_edge(edges[\"from\"][i],edges[\"to\"][i])\n",
    "          \n",
    "            \n",
    "#             #汇集强度\n",
    "            self.edges_strength.loc[[edges[\"from\"][i]],[edges[\"to\"][i]]]=edges[\"strength\"][i]\n",
    "        \n",
    "        return 1\n",
    "    def set_parment(self):\n",
    "        #从服务器中心get到网络结构\n",
    "        #初始化model\n",
    "        model.edges\n",
    "    \n",
    "        return \n",
    "    def update_model(self,fuse_dag):\n",
    "        #代改动部分，每次也是创建一个新的贝叶斯网络实例进行白名单结构学习，并返回边模型，最好还是保持pgmpy的形式\n",
    "        \n",
    "        \n",
    "        self.model = DAG()\n",
    "        #设置白名单\n",
    "        white_list = fuse_dag.edges()\n",
    "        wl = []\n",
    "        for edge in white_list:\n",
    "            wl.append([edge[0],edge[1]])\n",
    "        \n",
    "        wl = pd.DataFrame(wl,columns=[\"from\",\"to\"])\n",
    "        wl_r =ro.conversion.py2rpy(wl)\n",
    "        globalenv['wl_r'] = wl_r\n",
    "        self_r_data = ro.conversion.py2rpy(self.data)\n",
    "        globalenv['self_r_data'] = self.data\n",
    "        \n",
    "        r_script=\"\"\"\n",
    "        library(bnlearn)\n",
    "       \n",
    "        datacols = names(self_r_data)\n",
    "        for (i in 1:ncol(self_r_data)) { \n",
    "        if(i>45){\n",
    "            self_r_data[,datacols[i]] <- factor(self_r_data[,datacols[i]],levels = c(0,1))\n",
    "        }else{\n",
    "            self_r_data[,datacols[i]] <- factor(self_r_data[,datacols[i]])\n",
    "              }\n",
    "        }\n",
    "        dag = hc(self_r_data,score = sc,blacklist = bl_r,whitelist = wl_r)\n",
    "        arcs = arc.strength(dag,self_r_data)\n",
    "        \n",
    "        \"\"\"\n",
    "        edges = r(r_script)\n",
    "        self.model_edges = edges\n",
    "      \n",
    "        for i in range(len(edges[\"from\"])):\n",
    "            self.model.add_edge(edges[\"from\"][i],edges[\"to\"][i])\n",
    "        \n",
    "\n",
    "        \n",
    "class sever():\n",
    "    def __init__(self,fuse_num):\n",
    "        self.fuse_num = fuse_num\n",
    "    def Fd_caculate(self,edges_strength_matrix):\n",
    "        #聚合方法\n",
    "        #聚合的方式改为计算强度的阈值，并返回一个有无环图\n",
    "        fuseDAG = DAG()\n",
    "        #变为list，并且设置为df格式\n",
    "        arc_ls = pd.DataFrame(to_strength_list(edges_strength_matrix.astype(np.float16)),columns=[\"from\",\"to\",\"strength\"])\n",
    "        #from和to的df格式并按大小排列\n",
    "        arc_ls = arc_ls.sort_values(by=\"strength\",ascending=False)\n",
    "        #1.按照10个选择\n",
    "        for  i in range(20):\n",
    "            fuseDAG.add_edge(arc_ls.loc[i,\"from\"],arc_ls.loc[i,\"to\"])\n",
    "            \n",
    "   \n",
    "        return  fuseDAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f850553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DB_1 = pd.read_csv(\"c:data/site_16topaucoutlinePmmImpData_dis.csv\").drop(columns=\"Unnamed: 0\",axis = 1)\n",
    "DB_1[\"label\"].value_counts()\n",
    "DB_1 = DB_1.astype(np.int16)\n",
    "\n",
    "# 数据分割\n",
    "hos_id = [420,142,122,435,390,227,195,243,403,141]\n",
    "client_data = []\n",
    "for hosid in hos_id:\n",
    "    client_data.append(DB_1[DB_1[\"hospitalid\"]==hosid ].iloc[:,:].drop(columns = [\"hospitalid\"],axis = 1))\n",
    "    \n",
    "feature = client_data[0].columns\n",
    "\n",
    "#设置黑名单\n",
    "bl = []\n",
    "for i in feature:\n",
    "    if i ==\"label\":\n",
    "        pass\n",
    "    bl.append([\"label\",i])\n",
    "for i in feature:\n",
    "    if i ==\"age\":\n",
    "        pass\n",
    "    bl.append([i,\"age\"])\n",
    "for i in feature:\n",
    "    if i ==\"sex\":\n",
    "        pass\n",
    "    bl.append([i,\"sex\"])\n",
    "for i in feature:\n",
    "    if i ==\"race\":\n",
    "        pass\n",
    "    bl.append([i,\"race\"])\n",
    "\n",
    "bl = pd.DataFrame(bl,columns=[\"from\",\"to\"])\n",
    "bl_r = ro.conversion.py2rpy(bl)\n",
    "globalenv['bl_r'] = bl_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e16a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算样本量比重\n",
    "D = 19388\n",
    "D_W = []\n",
    "for i in range(10):\n",
    "    D_W.append(len(client_data[i])/D)\n",
    "D_W = np.float16(D_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9fa618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(ed,DB,cid):\n",
    "    \n",
    "    #cid客户端号，寻找相应的训练数据\n",
    "    auc_score = np.zeros((10,1))\n",
    "    recall = np.zeros((10,1))\n",
    "    acc = np.zeros((10,1))\n",
    "# #    实例化对象\n",
    "#     model= BayesianNetwork()\n",
    "#     #创建边\n",
    "    \n",
    "#     model.add_edges_from(ed)\n",
    "#     #确保数据的每一个都是新的\n",
    "#     f_1 = model.nodes\n",
    "        \n",
    "#     f_2 = DB[cid].columns\n",
    "#     data =DB[cid].copy(deep=True)\n",
    "#     drop_name = []\n",
    "#     for j in f_2:\n",
    "#         if not j in f_1:\n",
    "#             drop_name.append(j)\n",
    "#     data = data.drop(columns = drop_name,axis = 1)\n",
    "    \n",
    "#     #分出训练集\n",
    "#     train_data,test_data = train_test_split(DB[cid],test_size=0.3,random_state=2)\n",
    "#     #训练\n",
    "#     model.fit(train_data,estimator=BE)\n",
    "    \n",
    "    for i in range(len(DB)): \n",
    "        \n",
    "        model_i= BayesianNetwork(ed.edges)\n",
    "        #创建边\n",
    "        data = DB[i]\n",
    "        \n",
    "        f_1 = model_i.nodes\n",
    "        \n",
    "        f_2 = data.columns\n",
    "        #所有mode有的节点\n",
    "        drop_name = []\n",
    "        for j in f_2:\n",
    "            if not j in f_1:\n",
    "                drop_name.append(j)        \n",
    "        data = data.drop(columns = drop_name,axis = 1)\n",
    "        \n",
    "        \n",
    "        #创建离散变量说明\n",
    "        this_dict = []\n",
    "\n",
    "        for key,value in zip(state_names.keys(),state_names.values()):\n",
    "            \n",
    "            if key in f_1:\n",
    "\n",
    "                this_dict.append((key,value))\n",
    "        this_dict = dict(this_dict)\n",
    "         #训练模型\n",
    "        \n",
    "        train_data,test_data = train_test_split(data,test_size=0.3,random_state=2)\n",
    "        #得到y-true\n",
    "        y_true = test_data.iloc[:,0]\n",
    "        \n",
    "        \n",
    "        model_i.fit(train_data,estimator=BE,complete_samples_only=True,state_names=this_dict)\n",
    "        \n",
    "        #丢弃label获得\n",
    "        \n",
    "        test_data.drop(columns=\"label\",axis =1,inplace=True)\n",
    "      \n",
    "        #将预测的值从df转换为list形式\n",
    "        \n",
    "        #在predict的时候不知道什么会影响结果\n",
    "        y_pred = list(model_i.predict(test_data).iloc[:,0])\n",
    "       \n",
    "       #模型预测\n",
    "        acc[i] = accuracy_score(y_true,y_pred)\n",
    "        recall[i] = recall_score(y_true, y_pred)\n",
    "        auc_score[i] = roc_auc_score(y_true,y_pred)\n",
    "    print(acc)\n",
    "    print(recall)\n",
    "    print(auc_score)\n",
    "    return acc,recall,auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6038a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将所有的离散状态记录下来，以字典的方式\n",
    "state_names  ={\n",
    "    \"lable\":[0,1],\n",
    "    \"age\":[1,2,3,4,5,6,7,8],\n",
    "    \"sex\":[1,2],\n",
    "    \"race\":[1,2,3,4,5],\n",
    "    \"bmi\":[1,2,3,4],\n",
    "    \"temperature\":[1,2,3],\n",
    "    \"heartrate\":[1,2,3],\n",
    "    \"respiration\":[1,2,3],\n",
    "    \"SBP\":[1,2,3,4,5],\n",
    "    \"DBP\":[1,2,3,4,5],\n",
    "    \"paSystolic\":[1,2,3],\n",
    "    \"paDiastolic\":[1,2,3],\n",
    "    \"paMean\":[1,2,3],\n",
    "}\n",
    "LAB_list = []\n",
    "for i in range(1,61):\n",
    "    LAB_list.append((\"LAB\"+str(i),[1,2,3]))\n",
    "COM_list = []\n",
    "for i in range(1,12):\n",
    "    COM_list.append((\"COM\"+str(i),[0,1]))\n",
    "PRO_list = []\n",
    "for i in range(1,10):\n",
    "    COM_list.append((\"PRO\"+str(i),[0,1]))\n",
    "MED_list = []\n",
    "for i in range(1,55):\n",
    "    MED_list.append((\"MED\"+str(i),[0,1]))\n",
    "state_names.update(LAB_list)\n",
    "state_names.update(COM_list)\n",
    "state_names.update(PRO_list)\n",
    "state_names.update(MED_list)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "587fa54c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化训练\n",
      "初始auc []\n",
      "初始recall []\n",
      "初始acc []\n",
      "第 1 次迭代\n",
      "更 新客户端\n",
      "第 2 次迭代\n",
      "更 新客户端\n",
      "第 3 次迭代\n",
      "更 新客户端\n",
      "第 4 次迭代\n",
      "更 新客户端\n",
      "第 5 次迭代\n",
      "更 新客户端\n",
      "第 6 次迭代\n",
      "更 新客户端\n",
      "第 7 次迭代\n",
      "更 新客户端\n",
      "第 8 次迭代\n",
      "更 新客户端\n",
      "第 9 次迭代\n",
      "更 新客户端\n",
      "第 10 次迭代\n",
      "更 新客户端\n"
     ]
    }
   ],
   "source": [
    "globalenv['sc'] = \"k2\"\n",
    "sth = pd.DataFrame(np.zeros((len(feature),len(feature))),index = feature,columns=feature).astype(np.int16)\n",
    "pandas2ri.activate()\n",
    "# 结构训练\n",
    "train_round = 10\n",
    "round_num = 0\n",
    "client_num = 10\n",
    "clients = []\n",
    "for i in range(10):\n",
    "# 创建客户端，并且传入数据和结构学习模型\n",
    "    clients.append(client(client_data[i],i,sth))\n",
    "# 初始化服务器\n",
    "sever_1 = sever(6)\n",
    "global_edges_matrix = pd.DataFrame(np.zeros((len(feature),len(feature))),index = feature,columns=feature).astype(np.int16)\n",
    "edges_strength_matrix = pd.DataFrame(np.zeros((len(feature),len(feature))),index = feature,columns=feature).astype(np.int16)\n",
    "while(round_num<train_round):\n",
    "    #利用二维矩阵传递边结构信息\n",
    "    auc_list = []\n",
    "    recall_list = []\n",
    "    acc_list = []\n",
    "    #开始训练\n",
    "    #重置矩阵,设置评分\n",
    "    global_edges_matrix.iloc[:,:] = 0\n",
    "    edges_strength_matrix.iloc[:,:] = 0\n",
    "    acc = 0\n",
    "    recall = 0\n",
    "    acu_score = 0\n",
    "    #第一轮，先本地客户端建立初始模型\n",
    "    if round_num == 0:\n",
    "        \n",
    "        print(\"初始化训练\")\n",
    "        for i in range(client_num):\n",
    "            clients[i].fit()\n",
    "\n",
    "            \n",
    "            #计算初始性能，计算轮数的性能\n",
    "# #             print(\"各个客户端的初始性能\")\n",
    "#             acc,recall,auc_score = cross_validation(clients[i].model,client_data,clients[i].cid)\n",
    "#             auc_list.append(acu_score)\n",
    "#             recall_list.append(recall)\n",
    "#             acc_list.append(acc)\n",
    "\n",
    "        print(\"初始auc\",auc_list)\n",
    "        print(\"初始recall\",recall_list)\n",
    "        print(\"初始acc\",acc_list)\n",
    "        auc_list = []\n",
    "        recall_list = []\n",
    "        acc_list = [] \n",
    "        \n",
    "    #get到每个客户端边强度的总和\n",
    "    arc_strength_list = []\n",
    "    arc_weight_list = []\n",
    "    #开始聚集客户端边的信息\n",
    "    for i in range(client_num):\n",
    "        #将客户端最好的模型边信息转换为矩阵形式\n",
    "        \n",
    "        global_edges_matrix+=to_matrix(clients[i].get_model().edges(),feature)\n",
    "        #将矩阵转换为列表形式，并strength按列合并数据\n",
    "        \n",
    "        arc_strength_list.append(np.float16(pd.DataFrame(clients[i].arc_strength_list)[\"strength\"].sum()))\n",
    "        \n",
    "#         edges_strength_matrix+=clients[i].edges_strength\n",
    "    #计算每个图像的权\n",
    "    sum_value = sum(arc_strength_list)\n",
    "    \n",
    "    for  i in range(client_num):\n",
    "        arc_weight_list.append(arc_strength_list[i]/sum_value)\n",
    "    #汇集带权重的边的信息\n",
    "    for i in range(client_num):\n",
    "        edges_strength_matrix+=clients[i].edges_strength*arc_weight_list[i]*D_W[i]\n",
    "    \n",
    "    #服务器开始汇集信息，并且将矩阵转换为Dag\n",
    "#     print(\"未过滤的融合矩阵\")\n",
    "#     for index in edges_matrix.index:\n",
    "#         for col in edges_matrix.columns:\n",
    "#             if edges_matrix.loc[index,col]!=0:\n",
    "#                 print(index,\"->\",col,\"[label=\",edges_matrix.loc[index,col],\"];\")\n",
    "    \n",
    "    fuseDAG = sever_1.Fd_caculate(edges_strength_matrix)\n",
    "    print(\"第\",round_num+1,\"次迭代\")\n",
    "    #输出融合模型\n",
    "#     print_dag(fuseDAG)\n",
    "    #更新客户端模型\n",
    "    print(\"更 新客户端\")\n",
    "    #更新客户端，就是将融合的DAG()，传给各个模型，并用bl从新训练\n",
    "    \n",
    "    for i in range(client_num):\n",
    "        clients[i]. update_model(fuseDAG)\n",
    "        \n",
    "    round_num+=1\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe1ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAB19 -> label ;\n",
      "LAB19 -> COM5 ;\n",
      "LAB19 -> COM7 ;\n",
      "LAB19 -> LAB34 ;\n",
      "COM1 -> label ;\n",
      "COM1 -> COM11 ;\n",
      "COM1 -> COM7 ;\n",
      "COM1 -> PRO2 ;\n",
      "COM2 -> label ;\n",
      "COM2 -> COM8 ;\n",
      "COM2 -> LAB25 ;\n",
      "COM2 -> LAB3 ;\n",
      "COM3 -> label ;\n",
      "COM4 -> label ;\n",
      "COM4 -> COM10 ;\n",
      "COM4 -> LAB23 ;\n",
      "COM5 -> label ;\n",
      "COM5 -> COM11 ;\n",
      "COM5 -> COM8 ;\n",
      "COM5 -> PRO7 ;\n",
      "COM5 -> COM7 ;\n",
      "COM6 -> label ;\n",
      "COM6 -> PRO3 ;\n",
      "COM6 -> COM10 ;\n",
      "COM6 -> LAB36 ;\n",
      "COM6 -> LAB23 ;\n",
      "COM7 -> label ;\n",
      "COM7 -> COM8 ;\n",
      "COM7 -> COM10 ;\n",
      "COM7 -> COM11 ;\n",
      "COM7 -> PRO3 ;\n",
      "COM8 -> label ;\n",
      "COM9 -> label ;\n",
      "COM9 -> LAB23 ;\n",
      "COM9 -> LAB14 ;\n",
      "COM9 -> LAB35 ;\n",
      "COM9 -> LAB56 ;\n",
      "COM9 -> COM6 ;\n",
      "COM10 -> label ;\n",
      "COM10 -> COM3 ;\n",
      "COM10 -> PRO8 ;\n",
      "COM11 -> label ;\n",
      "COM11 -> COM10 ;\n",
      "COM11 -> PRO8 ;\n",
      "PRO1 -> label ;\n",
      "PRO1 -> LAB3 ;\n",
      "PRO1 -> PRO3 ;\n",
      "PRO1 -> COM1 ;\n",
      "PRO1 -> COM8 ;\n",
      "PRO1 -> COM7 ;\n",
      "PRO1 -> LAB10 ;\n",
      "PRO1 -> LAB19 ;\n",
      "PRO2 -> label ;\n",
      "PRO2 -> COM4 ;\n",
      "PRO2 -> PRO8 ;\n",
      "PRO2 -> PRO3 ;\n",
      "PRO3 -> label ;\n",
      "PRO3 -> PRO8 ;\n",
      "PRO3 -> COM8 ;\n",
      "PRO3 -> LAB56 ;\n",
      "PRO3 -> LAB25 ;\n",
      "PRO4 -> label ;\n",
      "PRO4 -> PRO7 ;\n",
      "PRO4 -> COM8 ;\n",
      "PRO4 -> COM7 ;\n",
      "PRO5 -> label ;\n",
      "PRO6 -> label ;\n",
      "PRO6 -> PRO7 ;\n",
      "PRO6 -> PRO8 ;\n",
      "PRO6 -> LAB56 ;\n",
      "PRO6 -> COM11 ;\n",
      "PRO6 -> LAB25 ;\n",
      "PRO6 -> LAB18 ;\n",
      "PRO6 -> COM5 ;\n",
      "PRO6 -> LAB46 ;\n",
      "PRO7 -> label ;\n",
      "PRO7 -> COM7 ;\n",
      "PRO7 -> PRO3 ;\n",
      "PRO7 -> PRO2 ;\n",
      "PRO7 -> COM11 ;\n",
      "PRO7 -> PRO9 ;\n",
      "PRO8 -> label ;\n",
      "PRO8 -> COM8 ;\n",
      "LAB48 -> LAB46 ;\n",
      "LAB48 -> LAB56 ;\n",
      "LAB46 -> LAB47 ;\n",
      "LAB46 -> LAB56 ;\n",
      "LAB36 -> LAB23 ;\n",
      "LAB23 -> LAB18 ;\n",
      "LAB34 -> LAB15 ;\n",
      "LAB34 -> LAB4 ;\n",
      "LAB15 -> LAB25 ;\n",
      "LAB15 -> LAB4 ;\n",
      "LAB15 -> LAB20 ;\n",
      "LAB15 -> LAB14 ;\n",
      "LAB59 -> LAB12 ;\n",
      "LAB12 -> LAB56 ;\n",
      "LAB12 -> LAB35 ;\n",
      "LAB12 -> COM10 ;\n",
      "LAB47 -> LAB56 ;\n",
      "LAB10 -> LAB59 ;\n",
      "LAB10 -> LAB12 ;\n",
      "LAB25 -> LAB3 ;\n",
      "LAB25 -> LAB4 ;\n",
      "LAB4 -> LAB14 ;\n",
      "LAB56 -> COM8 ;\n",
      "LAB3 -> COM11 ;\n",
      "LAB20 -> LAB10 ;\n",
      "LAB20 -> LAB23 ;\n",
      "LAB14 -> LAB20 ;\n",
      "age -> LAB19 ;\n",
      "age -> COM1 ;\n",
      "LAB18 -> LAB48 ;\n",
      "LAB35 -> LAB36 ;\n",
      "PRO9 -> PRO2 ;\n",
      "PRO9 -> LAB56 ;\n",
      "PRO9 -> COM1 ;\n",
      "PRO9 -> COM8 ;\n"
     ]
    }
   ],
   "source": [
    "print_dag(clients[8].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4f0cc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7da6a1c77f4966a0d3cc3449d74075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743e7a7e17de43d397218c6a752d812f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dface001ca574ac9b0aad7e1c55c45fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5488c4503ac442df8c7ac12128b7ce98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab069814b4a04afc949109884e2ecfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6cad113bef470b919e1abe951b6d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576557308e42473aad5b594f7ea499c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab63b7eaee64b4294d6f763640b43cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c999ab031fe6454296d8936031cfa289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/317 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5124dc601e084adab5aea915c27dce34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85046729]\n",
      " [0.82617587]\n",
      " [0.84139785]\n",
      " [0.78005115]\n",
      " [0.7942029 ]\n",
      " [0.81113801]\n",
      " [0.72485207]\n",
      " [0.76328052]\n",
      " [0.7318612 ]\n",
      " [0.82476636]]\n",
      "[[0.72972973]\n",
      " [0.71895425]\n",
      " [0.77294686]\n",
      " [0.86725664]\n",
      " [0.76691729]\n",
      " [0.74107143]\n",
      " [0.73913043]\n",
      " [0.40967742]\n",
      " [0.31578947]\n",
      " [0.72268908]]\n",
      "[[0.81453664]\n",
      " [0.79697712]\n",
      " [0.82036542]\n",
      " [0.80593048]\n",
      " [0.78911902]\n",
      " [0.78914037]\n",
      " [0.72706522]\n",
      " [0.65831184]\n",
      " [0.61284969]\n",
      " [0.79338337]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a8e726433e43f083c4c0822a3043c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#测试更新后的性能\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     print(\"各个客户端的初始性能\")\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     acc,recall,auc_score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclient_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     auc_list\u001b[38;5;241m.\u001b[39mappend(auc_score)\n\u001b[0;32m      9\u001b[0m     recall_list\u001b[38;5;241m.\u001b[39mappend(recall)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(ed, DB, cid)\u001b[0m\n\u001b[0;32m     65\u001b[0m  test_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m,axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m  \u001b[38;5;66;03m#将预测的值从df转换为list形式\u001b[39;00m\n\u001b[0;32m     68\u001b[0m  \n\u001b[0;32m     69\u001b[0m  \u001b[38;5;66;03m#在predict的时候不知道什么会影响结果\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m  y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mmodel_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m#模型预测\u001b[39;00m\n\u001b[0;32m     73\u001b[0m  acc[i] \u001b[38;5;241m=\u001b[39m accuracy_score(y_true,y_pred)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pgmpy\\models\\BayesianNetwork.py:734\u001b[0m, in \u001b[0;36mBayesianNetwork.predict\u001b[1;34m(self, data, stochastic, n_jobs)\u001b[0m\n\u001b[0;32m    731\u001b[0m pred_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    733\u001b[0m \u001b[38;5;66;03m# Send state_names dict from one of the estimated CPDs to the inference class.\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m pred_values \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_unique\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_unique\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(pred_values, index\u001b[38;5;241m=\u001b[39mdata_unique\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m    746\u001b[0m data_with_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data_unique, df_results], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "auc_list = []\n",
    "recall_list = []\n",
    "acc_list = []\n",
    "#测试更新后的性能\n",
    "for i in range(10):\n",
    "#     print(\"各个客户端的初始性能\")\n",
    "    acc,recall,auc_score = cross_validation(clients[i].model,client_data,clients[i].cid)\n",
    "    auc_list.append(auc_score)\n",
    "    recall_list.append(recall)\n",
    "    acc_list.append(acc)\n",
    "#     print(i)\n",
    "#     print_dag(clients[i].model)\n",
    "# print(\"更新auc\",auc_list)\n",
    "# print(\"更新recall\",recall_list)\n",
    "# print(\"更新acc\",acc_list)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f124a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list = np.atleast_2d(auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01b88013",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(10, 10, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m auc_result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauc_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py:331\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    326\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_prep_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_equal(values\u001b[38;5;241m.\u001b[39mdtype, dtype):\n\u001b[0;32m    334\u001b[0m     shape \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py:591\u001b[0m, in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    589\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(10, 10, 1)"
     ]
    }
   ],
   "source": [
    "auc_result = pd.DataFrame(auc_list).round(4)   \n",
    "# recall_result = pd.DataFrame(recall_list).round(4)\n",
    "# acc_result = pd.DataFrame(acc_list).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e08751",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_result.to_csv(\"C:/Users/Administrator/Desktop/code/project_2/结果/auc.csv\")\n",
    "recall_result.to_csv(\"C:/Users/Administrator/Desktop/code/project_2/结果/recall.csv\")\n",
    "acc_result.to_csv(\"C:/Users/Administrator/Desktop/code/project_2/结果/acc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a04d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAB19 -> label ;\n",
      "COM1 -> label ;\n",
      "COM2 -> label ;\n",
      "COM3 -> label ;\n",
      "COM4 -> label ;\n",
      "COM5 -> label ;\n",
      "COM6 -> label ;\n",
      "COM7 -> label ;\n",
      "COM8 -> label ;\n",
      "COM9 -> label ;\n"
     ]
    }
   ],
   "source": [
    "for i  in fuseDAG.edges():\n",
    "    print(i[0],\"->\",i[1],\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dag(fuseDAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(client_num):\n",
    "    #将客户端最好的模型边信息转换为矩阵形式\n",
    "    edges_matrix+=to_matrix(clients[i].get_model().edges(),feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaebab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"第\",i)\n",
    "    print_dag(clients[i].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d38344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data420.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data142.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data122.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data435.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data390.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data227.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data195.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data243.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data403.csv\n",
      "C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data141.csv\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(hos_id,client_data):\n",
    "    path = \"C:/Users/Administrator/Desktop/代码/实验/fd/fd_data/data\"+str(i)+\".csv\"\n",
    "    print(path)\n",
    "    j.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d88ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
